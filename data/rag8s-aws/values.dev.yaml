# Dev values - minimal defaults. All major features disabled by default.
# Keeps minimal resource requests so the whole stack (if enabled) stays well under ~16 GiB RAM.
global:
  labels: {}

aws:
  accountId: ""

iam:
  roleName: ""

karpenter:
  enabled: false
  provider:
    subnetSelector:
      karpenter.sh/discovery: rag8s-eks
    securityGroupSelector:
      karpenter.sh/discovery: rag8s-eks
  gpu:
    weight: 50
    instanceTypes:
      - p3.2xlarge
      - p4d.24xlarge
    capacityTypes:
      - spot
      - on-demand
    amiFamily: AL2
    tags:
      workload: gpu
      env: dev
    limits:
      # Reduced from massive values to something reasonable for a small dev cluster.
      # These are cluster-level limits for autoscaler decisions; keep them small for dev.
      cpu: "64"
      memory: "256Gi"
    ttlSecondsAfterEmpty: 60
  cpu:
    weight: 40
    instanceTypes:
      - m6i.large
    capacityTypes:
      - spot
      - on-demand
    amiFamily: AL2
    tags:
      workload: cpu
      env: dev
    limits:
      cpu: "64"
      memory: "256Gi"
    ttlSecondsAfterEmpty: 60

ray:
  enabled: true
  image:
    repository: "rayproject/ray"
    tag: "2.48.0"
    pullPolicy: IfNotPresent
  head:
    # reasonable tiny head to run ray control plane in dev
    cpu:
      request: "200m"
      limit: "500m"
    memory:
      request: "512Mi"
      limit: "1Gi"

rayservices:
  vllm:
    enabled: false
    name: rag8s-vllm-serve
    namespace: llm
    serviceAccountName: ray-inference-sa
    modelMountPath: "/opt/models"
    runtimeWorkingDir: "/app"
    importPath: "vllm.entrypoints.openai.api_server:app"
    env: []
    imagePullSecrets: []
    ray:
      version: "2.48.0"
      image:
        repository: "rayproject/ray"
        tag: "2.48.0"
        pullPolicy: IfNotPresent
      head:
        cpu:
          request: "300m"
          limit: "1"
        memory:
          request: "1Gi"
          limit: "2Gi"
        nodeSelector: {}
        tolerations: []
      workers:
        # keep a single worker capped to 1 replica for dev - increase if you have more memory
        cpu:
          request: "500m"
          limit: "1"
        memory:
          request: "2Gi"
          limit: "4Gi"
        gpu:
          request: 0
          limit: 0
        minReplicas: 0
        maxReplicas: 1
    worker:
      nodeSelector: {}
      tolerations: []
      affinity: {}
      annotations: {}
    head:
      nodeSelector: {}
      tolerations: []
      affinity: {}
      annotations: {}
    vllm:
      image:
        repository: "yourregistry/vllm"
        tag: "latest"
        pullPolicy: IfNotPresent
      model: ""
      tensorParallelSize: 1
      args: []
      env: []
    service:
      numGPUs: 0
      autoscale:
        minReplicas: 0
        maxReplicas: 1
        # targetRequestsPerReplica irrelevant for single-replica dev but left conservative
        targetRequestsPerReplica: 5

  onnxEmbedderReranker:
    enabled: false
    name: rag8s-embedder-reranker-rayserve-cpu-amd64
    namespace: inference
    serviceAccountName: ray-inference-sa
    ray:
      version: "2.48.0"
      image:
        repository: "rag8s/rag8s-onnx-embedder-reranker-cpu-amd64"
        tag: "gte-modernbert"
        pullPolicy: IfNotPresent
      head:
        cpu:
          request: "100m"
          limit: "250m"
        memory:
          request: "128Mi"
          limit: "256Mi"
      workers:
        cpu:
          request: "250m"
          limit: "500m"
        memory:
          request: "512Mi"
          limit: "1Gi"
        minReplicas: 0
        maxReplicas: 1
    embedder:
      numCPUs: 1
      autoscale:
        minReplicas: 0
        maxReplicas: 1
    reranker:
      numCPUs: 1
      autoscale:
        minReplicas: 0
        maxReplicas: 1
    importPath: "rayserve-embedder-reranker"
    runtimeWorkingDir: "/app"

rayjobs:
  enabled: false
  namespace: indexing
  indexing:
    name: rag8s-indexing
    image: myrepo/rag8s-indexer:latest
    entrypoint: "python scripts/index_documents.py --source s3://my-bucket"
    workingDir: "s3://my-bucket/code"
    pip:
      - qdrant-client
      - pandas
      - arango
    ttlSecondsAfterFinished: 3600
    rayVersion: "2.48.0"
    workerReplicas: 0
    workerMin: 0
    workerMax: 1
    resources:
      head:
        cpu: "200m"
        memory: "512Mi"
      worker:
        cpu: "500m"
        memory: "1Gi"

# Top-level Arango kept, but disabled for dev and trimmed to tiny footprint
arangodb:
  enabled: false
  image:
    repository: "arangodb/arangodb"
    tag: "3.12.5"
    pullPolicy: IfNotPresent
  authentication:
    jwtSecretName: "arango-cluster-jwt"
  agents:
    count: 1
    resources:
      requests:
        cpu: "100m"
        memory: "256Mi"
  coordinators:
    count: 1
    resources:
      requests:
        cpu: "200m"
        memory: "512Mi"
  dbservers:
    count: 1
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"
    podTemplate:
      nodeSelector: {}
      tolerations: []
  storage:
    engine: "rocksdb"
    persistentVolume:
      storageClassName: "standard"
      size: "10Gi"
  collectionsDefaults:
    numberOfShards: 1
    replicationFactor: 1

argocd:
  enabled: false
  namespace: argocd
  project: default
  repoURL: ""
  revision: HEAD
  path: infra/charts/rag8s-aws
  clusterURL: https://kubernetes.default.svc
  prune: true
  selfHeal: true
  override: {}

core:
  pdb:
    embedderReranker:
      enabled: true
      minAvailable: 1
    vllm:
      enabled: true
      minAvailable: 1
    frontend:
      enabled: true
      minAvailable: 1
    arangodb:
      enabled: true
      minAvailable: 1
    valkey:
      enabled: true
      minAvailable: 1

  quotas:
    inference:
      enabled: true
      # per-pod request: plan for somewhat heavy inference pods but allow only 2 pods at once
      requestsCpu: "500m"
      requestsMemory: "2Gi"
      limitsCpu: "1"
      limitsMemory: "4Gi"
      pods: "2"
      requestsGPUs: "0"

    indexing:
      enabled: false
      requestsCpu: "50m"
      requestsMemory: "128Mi"
      limitsCpu: "100m"
      limitsMemory: "256Mi"
      pods: "0"
    monitoring:
      enabled: false
      requestsCpu: "50m"
      requestsMemory: "128Mi"
      limitsCpu: "100m"
      limitsMemory: "256Mi"
      pods: "0"
    stateful:
      enabled: false
      requestsCpu: "50m"
      requestsMemory: "128Mi"
      limitsCpu: "100m"
      limitsMemory: "256Mi"
      pods: "0"
      pvcs: "0"
      storage: "1Gi"

  arangodb:
    enabled: false
    image:
      repository: "arangodb/arangodb"
      tag: "3.12.5"
      pullPolicy: IfNotPresent
    authentication:
      jwtSecretName: "arango-cluster-jwt"
    agents:
      count: 1
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
    coordinators:
      count: 1
      resources:
        requests:
          cpu: "200m"
          memory: "512Mi"
    dbservers:
      count: 1
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
      podTemplate:
        nodeSelector: {}
        tolerations: []
    storage:
      engine: "rocksdb"
      persistentVolume:
        storageClassName: "standard"
        size: "10Gi"
    collectionsDefaults:
      numberOfShards: 1
      replicationFactor: 1

  frontend:
    enabled: false
    replicas: 1
    image:
      repository: "rag8s/frontend"
      tag: "latest"
      pullPolicy: IfNotPresent
    port: 8080
    readiness:
      path: /health
      port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    liveness:
      path: /health
      port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
    resources:
      requests:
        cpu: "200m"
        memory: "256Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
    hpa:
      minReplicas: 0
      maxReplicas: 1
      cpuTargetPercentage: 60
      memoryTargetPercentage: 70
    service:
      type: ClusterIP
      port: 80
    configMap: rag8s-config
    secret: rag8s-secrets
    serviceAccount: frontend-sa

  valkey:
    enabled: false
    replicas: 0
    serviceAccount: "default"
    image:
      repository: "valkey/valkey"
      tag: "latest"
      pullPolicy: IfNotPresent
    maxmemory: "512mb"
    appendonly: "no"
    persistence:
      enabled: false
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "250m"
        memory: "256Mi"
    pod:
      replicas: 0
      stateful: false
    nodeSelector: {}
    tolerations: []
    affinity: {}

namespaces:
  inference: inference
  indexing: indexing
  monitoring: monitoring
  networking: networking
  arangodb: arangodb

serviceAccounts:
  ray-inference-sa:
    name: ray-inference-sa
    namespace: inference
    iam:
      roleName: ""
  frontend-sa:
    name: frontend-sa
    namespace: inference
    iam:
      roleName: ""

network:
  traefik:
    namespace: ""
    replicas: 0
    image:
      repository: traefik
      tag: v3.1
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      ports:
        - name: web
          port: 80
          targetPort: 8000
        - name: websecure
          port: 443
          targetPort: 8443
      annotations: {}
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "250m"
        memory: "256Mi"
    nodeSelector: {}
    tolerations: []
    affinity: {}
  ingress:
    namespace: ""
    class: traefik
    tls: []
    rules: []
  policies:
    enabled: false
    rules: []

monitoring:
  enabled: false
  namespace: monitoring
  scrapeInterval: "30s"
  prometheus:
    enabled: false
    serviceMonitor:
      enabled: false
      interval: "30s"
  grafana:
    enabled: false
    prometheusURL: "http://prometheus.monitoring.svc.cluster.local:9090"
    dashboards: {}
    datasources: []
  serviceMonitors: []
  alerts:
    enabled: false
    groups: []

arangobackup:
  enabled: false
  schedule: "0 */6 * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  backoffLimit: 1
  image:
    repository: "arangodb/arangodb"
    tag: "3.11.8"
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: "100m"
      memory: "128Mi"
    limits:
      cpu: "250m"
      memory: "256Mi"
  pvcClaim: "arangodb-backup-pvc"
  rcloneSecret: "rclone-config"
  serviceAccount: "default"
  nodeSelector: {}
  tolerations: []
  aws:
    region: "us-east-1"
    bucket: ""
    prefix: "arangodb"
  arango:
    endpoint: "tcp://arangodb.default.svc.cluster.local:8529"
    secret: "arangodb-credentials"
  retention:
    days: 7

valkey:
  enabled: false
  replicas: 0
  serviceAccount: "default"
  image:
    repository: "valkey/valkey"
    tag: "latest"
    pullPolicy: IfNotPresent
  maxmemory: "512mb"
  appendonly: "no"
  persistence:
    enabled: false
  resources:
    requests:
      cpu: "100m"
      memory: "128Mi"
    limits:
      cpu: "250m"
      memory: "256Mi"
  pod:
    replicas: 0
    stateful: false
  nodeSelector: {}
  tolerations: []
  affinity: {}

extras: {}

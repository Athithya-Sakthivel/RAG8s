# values.yaml - final
global:
  namespace: ""
  labels: {}

aws:
  accountId: ""

iam:
  roleName: ""

karpenter:
  enabled: false
  provider:
    subnetSelector:
      karpenter.sh/discovery: rag8s-eks
    securityGroupSelector:
      karpenter.sh/discovery: rag8s-eks
  gpu:
    weight: 50
    instanceTypes:
      - p3.2xlarge
      - p4d.24xlarge
    capacityTypes:
      - spot
      - on-demand
    amiFamily: AL2
    tags:
      workload: gpu
      env: dev
    limits:
      cpu: "256"
      memory: "1024Gi"
    ttlSecondsAfterEmpty: 60
  cpu:
    weight: 40
    instanceTypes:
      - m6i.large
    capacityTypes:
      - spot
      - on-demand
    amiFamily: AL2
    tags:
      workload: cpu
      env: dev
    limits:
      cpu: "512"
      memory: "2048Gi"
    ttlSecondsAfterEmpty: 60

ray:
  enabled: true
  image:
    repository: "rayproject/ray"
    tag: "2.10.0"
    pullPolicy: IfNotPresent
  head:
    cpu:
      request: "1000m"
      limit: "2000m"
    memory:
      request: "4Gi"
      limit: "8Gi"

rayservices:
  vllm:
    enabled: true
    name: rag8s-vllm-serve
    namespace: llm
    serviceAccountName: ray-inference-sa
    modelMountPath: "/opt/models"
    runtimeWorkingDir: "/app"
    importPath: "vllm.entrypoints.openai.api_server:app"
    env:
      - name: LOG_LEVEL
        value: "INFO"
      - name: OTEL_SERVICE_NAME
        value: "vllm-service"
    imagePullSecrets: []
    ray:
      version: "2.10.0"
      image:
        repository: "rayproject/ray"
        tag: "2.10.0"
        pullPolicy: IfNotPresent
      head:
        cpu:
          request: "1000m"
          limit: "2000m"
        memory:
          request: "4Gi"
          limit: "8Gi"
        nodeSelector: {}
        tolerations: []
      workers:
        cpu:
          request: "1000m"
          limit: "2000m"
        memory:
          request: "8Gi"
          limit: "16Gi"
        gpu:
          request: 1
          limit: 1
        minReplicas: 1
        maxReplicas: 4
    worker:
      nodeSelector:
        karpenter.sh/provisioner-name: "gpu-provisioner"
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      affinity: {}
      annotations: {}
    head:
      nodeSelector:
        karpenter.sh/provisioner-name: "cpu-provisioner"
      tolerations: []
      affinity: {}
      annotations: {}
    vllm:
      image:
        repository: "yourregistry/vllm"
        tag: "latest"
        pullPolicy: IfNotPresent
      model: "meta-llama/Llama-2-13b-chat-hf"
      tensorParallelSize: 1
      args: []
      env: []
    service:
      numGPUs: 1
      autoscale:
        minReplicas: 1
        maxReplicas: 4
        targetRequestsPerReplica: 5

  onnxEmbedderReranker:
    enabled: true
    name: rag8s-embedder-reranker-serve
    namespace: llm
    serviceAccountName: ray-inference-sa
    ray:
      version: "2.10.0"
      image:
        repository: rayproject/ray
        tag: "2.10.0"
        pullPolicy: IfNotPresent
      head:
        cpu:
          request: "500m"
          limit: "1"
        memory:
          request: "1Gi"
          limit: "2Gi"
        nodeSelector:
          karpenter.sh/provisioner-name: cpu-provisioner
          kubernetes.io/arch: amd64
        tolerations: []
      workers:
        cpu:
          request: "500m"
          limit: "1"
        memory:
          request: "1Gi"
          limit: "1Gi"
        minReplicas: 1
        maxReplicas: 4
        nodeSelector:
          karpenter.sh/provisioner-name: cpu-provisioner
          kubernetes.io/arch: amd64
        tolerations: []
    imagePullSecrets: []
    modelMountPath: "/opt/models"
    useHostPath: false
    hostPath: ""
    usePVC: false
    modelPVC: ""
    env: {}
    head:
      annotations: {}
      nodeSelector:
        karpenter.sh/provisioner-name: cpu-provisioner
        kubernetes.io/arch: amd64
      tolerations: []
      affinity: {}
    worker:
      minReplicas: 1
      maxReplicas: 4
      annotations: {}
      nodeSelector:
        karpenter.sh/provisioner-name: cpu-provisioner
        kubernetes.io/arch: amd64
      tolerations: []
      affinity: {}
    embedder:
      numCPUs: 1
      autoscale:
        minReplicas: 1
        maxReplicas: 4
        targetRequestsPerReplica: 10
    reranker:
      numCPUs: 1
      autoscale:
        minReplicas: 1
        maxReplicas: 2
        targetRequestsPerReplica: 5
    importPath: "rayserve-embedder-reranker"
    runtimeWorkingDir: "/app"

rayjobs:
  namespace: indexing
  indexing:
    name: rag8s-indexing
    image: myrepo/rag8s-indexer:latest
    entrypoint: "python scripts/index_documents.py --source s3://my-bucket"
    workingDir: "s3://my-bucket/code"
    pip:
      - qdrant-client
      - pandas
      - arango
    ttlSecondsAfterFinished: 3600
    rayVersion: "2.10.0"
    workerReplicas: 1
    workerMin: 1
    workerMax: 1
    resources:
      head:
        cpu: "250m"
        memory: "512Mi"
      worker:
        cpu: "250m"
        memory: "512Mi"

# Keep top-level arangodb for any templates referencing it directly
arangodb:
  enabled: true
  image:
    repository: "arangodb/arangodb"
    tag: "3.12.5"
    pullPolicy: IfNotPresent
  authentication:
    jwtSecretName: "arango-cluster-jwt"
  agents:
    count: 3
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"
  coordinators:
    count: 2
    resources:
      requests:
        cpu: "1000m"
        memory: "2Gi"
  dbservers:
    count: 3
    resources:
      requests:
        cpu: "4000m"
        memory: "16Gi"
    podTemplate:
      nodeSelector:
        node.kubernetes.io/instance-type: "c8gd"
      tolerations:
        - key: "node.kubernetes.io/unschedulable"
          operator: "Exists"
          effect: "NoSchedule"
  storage:
    engine: "rocksdb"
    persistentVolume:
      storageClassName: "local-nvme"
      size: "500Gi"
  collectionsDefaults:
    numberOfShards: 8
    replicationFactor: 3

argocd:
  enabled: false
  namespace: argocd
  project: default
  repoURL: ""
  revision: HEAD
  path: infra/charts/rag8s-aws
  clusterURL: https://kubernetes.default.svc
  prune: true
  selfHeal: true
  override: {}

core:
  pdb:
    embedderReranker:
      enabled: false
      minAvailable: 1
    vllm:
      enabled: false
      minAvailable: 0
    frontend:
      enabled: false
      minAvailable: 1
    arangodb:
      enabled: true
      minAvailable: 1
    valkey:
      enabled: true
      minAvailable: 1

  quotas:
    inference:
      enabled: true
      requestsCpu: "250m"
      requestsMemory: "512Mi"
      limitsCpu: "500m"
      limitsMemory: "1Gi"
      pods: "2"
      requestsGPUs: "0"
    indexing:
      enabled: true
      requestsCpu: "500m"
      requestsMemory: "512Mi"
      limitsCpu: "1"
      limitsMemory: "1Gi"
      pods: "1"
    monitoring:
      enabled: false
      requestsCpu: "100m"
      requestsMemory: "256Mi"
      limitsCpu: "250m"
      limitsMemory: "512Mi"
      pods: "1"
    stateful:
      enabled: true
      requestsCpu: "250m"
      requestsMemory: "256Mi"
      limitsCpu: "500m"
      limitsMemory: "512Mi"
      pods: "1"
      pvcs: "1"
      storage: "1Gi"

  arangodb:
    enabled: true
    image:
      repository: "arangodb/arangodb"
      tag: "3.12.5"
      pullPolicy: IfNotPresent
    authentication:
      jwtSecretName: "arango-cluster-jwt"
    agents:
      count: 3
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
    coordinators:
      count: 2
      resources:
        requests:
          cpu: "1000m"
          memory: "2Gi"
    dbservers:
      count: 3
      resources:
        requests:
          cpu: "4000m"
          memory: "16Gi"
      podTemplate:
        nodeSelector:
          node.kubernetes.io/instance-type: "c8gd"
        tolerations:
          - key: "node.kubernetes.io/unschedulable"
            operator: "Exists"
            effect: "NoSchedule"
    storage:
      engine: "rocksdb"
      persistentVolume:
        storageClassName: "local-nvme"
        size: "500Gi"
    collectionsDefaults:
      numberOfShards: 8
      replicationFactor: 3

  frontend:
    replicas: 2
    image:
      repository: "rag8s/frontend"
      tag: "latest"
      pullPolicy: IfNotPresent
    port: 8080
    readiness:
      path: /health
      port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    liveness:
      path: /health
      port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"
      limits:
        cpu: "1"
        memory: "2Gi"
    hpa:
      minReplicas: 2
      maxReplicas: 10
      cpuTargetPercentage: 60
      memoryTargetPercentage: 70
    service:
      type: ClusterIP
      port: 80
    configMap: rag8s-config
    secret: rag8s-secrets
    serviceAccount: frontend-sa

  valkey:
    enabled: true
    replicas: 1
    serviceAccount: "default"
    image:
      repository: "valkey/valkey"
      tag: "latest"
      pullPolicy: IfNotPresent
    maxmemory: "2gb"
    appendonly: "no"
    persistence:
      enabled: false
    resources:
      requests:
        cpu: "200m"
        memory: "512Mi"
      limits:
        cpu: "500m"
        memory: "1Gi"
    pod:
      replicas: 1
      stateful: false
    nodeSelector:
      storage: nvme
    tolerations: []
    affinity: {}

namespaces:
  inference: inference
  indexing: indexing
  monitoring: monitoring
  networking: networking
  arangodb: arangodb

serviceAccounts:
  ray-inference-sa:
    name: ray-inference-sa
    namespace: inference
    iam:
      roleName: ""
  frontend-sa:
    name: frontend-sa
    namespace: inference
    iam:
      roleName: ""

network:
  traefik:
    namespace: ""
    replicas: 1
    image:
      repository: traefik
      tag: v3.1
      pullPolicy: IfNotPresent
    service:
      type: LoadBalancer
      ports:
        - name: web
          port: 80
          targetPort: 8000
        - name: websecure
          port: 443
          targetPort: 8443
      annotations: {}
    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
  ingress:
    namespace: ""
    class: traefik
    tls: []
    rules: []
  policies:
    enabled: false
    rules: []

monitoring:
  enabled: true
  namespace: monitoring
  scrapeInterval: "30s"
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: "30s"
  grafana:
    enabled: true
    prometheusURL: "http://prometheus.monitoring.svc.cluster.local:9090"
    dashboards:
      ray-overview.json: "dashboards/ray-overview.json"
      arangodb-overview.json: "dashboards/arangodb-overview.json"
      eks-cluster.json: "dashboards/eks-cluster.json"
      traefik.json: "dashboards/traefik.json"
    datasources:
      - name: Prometheus
        type: prometheus
        url: "http://prometheus.monitoring.svc.cluster.local:9090"
  serviceMonitors: []
  alerts:
    enabled: true
    groups: []

arangobackup:
  schedule: "0 */6 * * *"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 5
  backoffLimit: 2
  image:
    repository: "arangodb/arangodb"
    tag: "3.11.8"
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "1Gi"
  pvcClaim: "arangodb-backup-pvc"
  rcloneSecret: "rclone-config"
  serviceAccount: "default"
  nodeSelector:
    storage: nvme
  tolerations: []
  aws:
    region: "us-east-1"
    bucket: "my-arangodb-backups"
    prefix: "arangodb"
  arango:
    endpoint: "tcp://arangodb.default.svc.cluster.local:8529"
    secret: "arangodb-credentials"
  retention:
    days: 30

valkey:
  enabled: true
  replicas: 1
  serviceAccount: "default"
  image:
    repository: "valkey/valkey"
    tag: "latest"
    pullPolicy: IfNotPresent
  maxmemory: "2gb"
  appendonly: "no"
  persistence:
    enabled: false
  resources:
    requests:
      cpu: "200m"
      memory: "512Mi"
    limits:
      cpu: "500m"
      memory: "1Gi"
  pod:
    replicas: 1
    stateful: false
  nodeSelector:
    storage: nvme
  tolerations: []
  affinity: {}

extras: {}

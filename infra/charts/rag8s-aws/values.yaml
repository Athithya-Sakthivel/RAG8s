core:
  pdb:
    embedderReranker:
      enabled: true
      minAvailable: 1
    sglang:
      enabled: true
      minAvailable: 1
    frontend:
      enabled: true
      minAvailable: 1
    qdrant:
      enabled: true
      minAvailable: 1
    arangodb:
      enabled: true
      minAvailable: 1

  quotas:
    inference:
      enabled: true
      requestsCpu: "16"
      requestsMemory: "64Gi"
      limitsCpu: "32"
      limitsMemory: "128Gi"
      pods: "50"
    indexing:
      enabled: true
      requestsCpu: "32"
      requestsMemory: "128Gi"
      limitsCpu: "64"
      limitsMemory: "256Gi"
      pods: "30"
    monitoring:
      enabled: true
      requestsCpu: "8"
      requestsMemory: "32Gi"
      limitsCpu: "16"
      limitsMemory: "64Gi"
      pods: "20"
    stateful:
      enabled: true
      requestsCpu: "64"
      requestsMemory: "256Gi"
      limitsCpu: "128"
      limitsMemory: "512Gi"
      pods: "30"
      pvcs: "50"
      storage: "10Ti"

  frontend:
    replicas: 2
    serviceAccount: frontend-sa
    image:
      repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/rag8s-frontend
      tag: "v1.0.0"
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"
      limits:
        cpu: "1"
        memory: "2Gi"
    hpa:
      minReplicas: 2
      maxReplicas: 10
      cpuTargetPercentage: 60
      memoryTargetPercentage: 70
    service:
      type: ClusterIP

monitoring:
  namespace: monitoring

  serviceMonitors:
    - name: frontend-metrics
      namespace: inference
      matchLabels:
        app.kubernetes.io/name: rag8s-frontend
      port: http
      path: /metrics
      interval: 15s
    - name: sglang-metrics
      namespace: inference
      matchLabels:
        app.kubernetes.io/name: sglang
      port: metrics
      interval: 15s
    - name: embedder-metrics
      namespace: inference
      matchLabels:
        app.kubernetes.io/name: embedder-reranker
      port: metrics
      interval: 15s
    - name: traefik-metrics
      namespace: networking
      matchLabels:
        app.kubernetes.io/name: traefik
      port: metrics
      interval: 30s
    - name: qdrant-metrics
      namespace: vector-db
      matchLabels:
        app.kubernetes.io/name: qdrant
      port: http
      interval: 30s
    - name: arangodb-metrics
      namespace: graph-db
      matchLabels:
        app.kubernetes.io/name: arangodb
      port: http
      interval: 30s

  grafana:
    prometheusURL: "http://prometheus-operated.monitoring.svc.cluster.local:9090"

  alerts:
    enabled: true
    groups:
      - name: workload-health
        interval: 30s
        rules:
          - alert: HighCPUUsage
            expr: sum(rate(container_cpu_usage_seconds_total{container!="",pod!=""}[5m])) by (pod) > 0.85
            for: 2m
            severity: critical
            summary: "High CPU usage detected"
            description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is using >85% CPU."
          - alert: HighMemoryUsage
            expr: sum(container_memory_usage_bytes{container!="",pod!=""}) by (pod) / sum(container_spec_memory_limit_bytes{container!="",pod!=""}) by (pod) > 0.9
            for: 2m
            severity: critical
            summary: "High memory usage detected"
            description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is using >90% memory."
      - name: ray-service-health
        rules:
          - alert: RayServiceDown
            expr: up{app_kubernetes_io_name="ray"} == 0
            for: 1m
            severity: critical
            summary: "RayService is down"
            description: "RayService {{ $labels.service }} in namespace {{ $labels.namespace }} is not responding."


network:
  traefik:
    namespace: networking
    replicas: 2
    service:
      type: LoadBalancer
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
        service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
      ports:
        - name: web
          port: 80
          targetPort: 8000
        - name: websecure
          port: 443
          targetPort: 8443
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi
    nodeSelector: {}
    tolerations: []
    affinity: {}

  ingress:
    namespace: networking
    rules:
      - host: frontend.example.com
        serviceName: rag8s-frontend
        servicePort: 80
      - host: api.example.com
        serviceName: sglang
        servicePort: 80
    tls:
      - secretName: tls-secret
        hosts:
          - frontend.example.com
          - api.example.com
   
  policies:
    enabled: true
    rules:
      - name: allow-traefik-to-inference
        namespace: networking
        podSelector:
          matchLabels:
            app.kubernetes.io/name: traefik
        ingress:
          - from:
              - namespaceSelector:
                  matchLabels:
                    name: inference
        egress: []
      - name: allow-inference-to-vector-db
        namespace: inference
        podSelector:
          matchLabels:
            app.kubernetes.io/name: rag8s-frontend
        egress:
          - to:
              - namespaceSelector:
                  matchLabels:
                    name: vector-db
        ingress: []
      - name: deny-all-default
        namespace: inference
        podSelector: {}
        ingress: []
        egress: []

rayservices:
  onnxEmbedderReranker:
    enabled: true
    name: rag8s-embedder-reranker-serve
    namespace: inference
    serviceAccountName: ray-inference-sa
    imagePullSecrets: []
    usePVC: false
    useHostPath: false
    hostPath: /opt/models
    modelPVC: embedder-model-pvc
    modelMountPath: /opt/models
    importPath: rayserve-embedder-reranker
    runtimeWorkingDir: /app
    env: []
    
    ray:
      version: "2.9.3"
      image:
        repository: myregistry/ray
        tag: 2.9.3
        pullPolicy: IfNotPresent
      head:
        cpu:
          request: "500m"
          limit: "1"
        memory:
          request: "1Gi"
          limit: "2Gi"
      workers:
        minReplicas: 1
        maxReplicas: 4
        cpu:
          request: "500m"
          limit: "1"
        memory:
          request: "1Gi"
          limit: "1Gi"

    embedder:
      numCPUs: 1
      autoscale:
        minReplicas: 1
        maxReplicas: 4
        targetRequestsPerReplica: 10

    reranker:
      numCPUs: 1
      autoscale:
        minReplicas: 1
        maxReplicas: 2
        targetRequestsPerReplica: 5

    head:
      annotations: {}
      nodeSelector: {}
      tolerations: []
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                        - embedder-reranker
                topologyKey: "kubernetes.io/hostname"

    worker:
      annotations: {}
      nodeSelector: {}
      tolerations: []
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                        - embedder-reranker
                topologyKey: "kubernetes.io/hostname"

  sglang:
    name: rag8s-sglang-serve
    namespace: inference
    ray:
      version: "2.35.0"
      image:
        repository: myrepo/sglang-ray
        tag: latest
        pullPolicy: IfNotPresent
      head:
        cpu:
          request: "1000m"
          limit: "2000m"
        memory:
          request: "4Gi"
          limit: "8Gi"
      workers:
        minReplicas: 1
        maxReplicas: 4
        cpu:
          request: "1000m"
          limit: "2000m"
        memory:
          request: "8Gi"
          limit: "16Gi"
        gpu:
          request: 1
          limit: 1
    modelMountPath: "/opt/models"
    service:
      numGPUs: 1
      autoscale:
        minReplicas: 1
        maxReplicas: 4
        targetRequestsPerReplica: 5


karpenter:
  provider:
    subnetSelector:
      karpenter.sh/discovery: rag8s-eks
    securityGroupSelector:
      karpenter.sh/discovery: rag8s-eks
  gpu:
    weight: 50
    instanceTypes:
      - p3.2xlarge
      - p4d.24xlarge
    capacityTypes:
      - spot
      - on-demand
    amiFamily: AL2
    tags:
      workload: gpu
      env: production
    limits:
      cpu: "256"
      memory: "1024Gi"
    ttlSecondsAfterEmpty: 60
  cpu:
    weight: 40
    instanceTypes:
      - c8g.2xlarge
      - m6i.large
    capacityTypes:
      - spot
      - on-demand
    amiFamily: AL2
    tags:
      workload: cpu
      env: production
    limits:
      cpu: "512"
      memory: "2048Gi"
    ttlSecondsAfterEmpty: 60


rayjobs:
  namespace: inference
  indexing:
    name: rag8s-indexing
    image: myrepo/rag8s-indexer:latest
    entrypoint: "python scripts/index_documents.py --source s3://my-bucket"
    workingDir: "s3://my-bucket/code"
    pip:
      - qdrant-client
      - pandas
      - arango
    ttlSecondsAfterFinished: 3600
    rayVersion: "2.10.0"
    workerReplicas: 2
    workerMin: 1
    workerMax: 4
    resources:
      head:
        cpu: "4"
        memory: "8Gi"
      worker:
        cpu: "8"
        memory: "16Gi"

qdrant:
  image:
    repository: qdrant/qdrant
    tag: "1.15.1"           # Pin to latest or specific stable version
    pullPolicy: IfNotPresent

  persistence:
    enabled: true
    storageClass: local-nvme
    accessModes:
      - ReadWriteOnce
    size: 500Gi            # Adjust based on dataset sizes

  config:
    service:
      http_port: 6333
      grpc_port: 6334
    storage:
      path: /qdrant/storage
    log_level: INFO
    wal_enabled: true
    memory:
      cache_size: 4096

  nodeSelector:
    eks.amazonaws.com/nodegroup: "stateful-nvme"

  tolerations: []         # Define if using taints on NVMe nodes

  resources:
    limits:
      cpu: "2"
      memory: "4Gi"
    requests:
      cpu: "1"
      memory: "2Gi"

arangodb:
  image:
    repository: arangodb/arangodb
    tag: "3.11.0"           # Example version, override as needed
    pullPolicy: IfNotPresent

  deploymentMode: Cluster    # Or Single depending on your scale

  storageClass: local-nvme
  size: 500Gi                # Persistent volume capacity

  resources:
    requests:
      cpu: "2"
      memory: "4Gi"
    limits:
      cpu: "4"
      memory: "8Gi"

  nodeSelector:
    eks.amazonaws.com/nodegroup: "stateful-nvme"

  tolerations: []            # If your NVMe nodes are tainted

  service:
    type: ClusterIP
    monitorPort: 8528        # If relevant for metrics/service monitor


rayjobs:
  namespace: inference
  indexing:
    name: rag8s-indexing
    image: myrepo/rag8s-indexer:latest
    entrypoint: "python scripts/index_documents.py --source s3://my-bucket"
    workingDir: "s3://my-bucket/code"
    pip:
      - qdrant-client
      - pandas
      - arango
    ttlSecondsAfterFinished: 3600
    rayVersion: "2.10.0"
    workerReplicas: 2
    workerMin: 1
    workerMax: 4
    resources:
      head:
        cpu: "4"
        memory: "8Gi"
      worker:
        cpu: "8"
        memory: "16Gi"

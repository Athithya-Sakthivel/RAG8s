

**NVIDIA (June 2025)** recommends **page-level chunking** as the most accurate and consistent baseline:

> “Page-level chunking achieved the highest average accuracy (0.648) with the lowest standard deviation (0.107)... It outperformed token- and section-level chunking.”
> — [NVIDIA Developer Blog](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses/?utm_source=chatgpt.com)

**Use page-wise as the default** for structured/unstructured docs.


| **Component**                         | **Tool(s)**                                          | **Exact Chunking Strategy**                                                                                                                                                                                           | **Why Chosen for Scalability**                                                                                                    |
| ------------------------------------- | ---------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| **Audio Transcription**               | `faster-whisper`, `pydub`, `ffmpeg-python`           | Audio is loaded using `pydub`; sliced into 20–30s segments based on silence threshold (`pydub.silence.detect_nonsilent`). Each segment becomes a chunk with `start_time`, `end_time`.                                 | `faster-whisper` (CTranslate2) enables real-time CPU/GPU inference. `ffmpeg` ensures format compatibility and slicing efficiency. |
| **HTML Parsing**                      | `extractous`                                         | HTML is parsed with `BeautifulSoup`; `<h*>`, `<p>`, `<section>` are treated as delimiters. Chunks are formed from contiguous text blocks under the same section or heading.                                           | Lightweight, preserves HTML hierarchy; chunking respects semantic structure for retrieval alignment.                              |
| **PDF Parsing + OCR**                 | `pdfplumber`, `PyMuPDF`, `paddleocr`, `paddlepaddle` | Default: 1 page = 1 chunk. If page is too sparse or OCR-detected, fallback to paragraph-based chunking using line height + spacing heuristics (`line_gap > 1.5x median`). OCR fallback kicks in for image-heavy PDFs. | Multilingual, layout-aware, reliable fallback. All parsing piped through unified structure into Ray pipelines.                    |
| **CSV Chunking**                      | `ray.data.read_csv()` + `.window()`                  | `ray.data.read_csv(path)` → auto schema detection. Chunked using: `ds.window(bytes_per_window=N)` where `N = max(5120, avg_row_len * CSV_ROWS_PER_CHUNK)`. `CSV_ROWS_PER_CHUNK = ceil(2048 / avg_char_per_row)`       | Windowed loading avoids memory spikes. Scales across files and cores. Streaming support for massive CSVs.                         |
| **JSON Chunking**                     | `ray.data.read_json()` + `.window()`                 | For JSON Lines: each line = record. Chunked using `ds.window(size=RECORDS_PER_CHUNK)`. `RECORDS_PER_CHUNK = ceil(4096 / avg_chars_per_record)`. For nested JSON: flatten → explode arrays → chunk by depth grouping.  | Handles deeply nested JSON using recursive flattening. Dynamically adapts to record size and depth.                               |
| **Pipeline Orchestration**            | `ray` (core, actors, tasks)                          | Each chunking stage is a Ray task/actor. File routing logic handled by `ray.remote(main_dispatcher)`. Parallel across cores or Ray cluster nodes.                                                                     | Ray enables both local & distributed mode. Shared object store improves I/O and communication between stages.                     |
| **Model Serving / Inference**         | `sglang`                                             | Each chunk processed asynchronously via batched inference pipeline: `embedding`, `reranking`, `entity linking`. Response includes latency + output fields (`entities`, `triplets`).                                   | Multi-model engine. Supports CPU inference or GPU fallback. Token streaming for long chunk tolerance.                             |
| **Main Parser Entry Point**           | `indexing_pipeline/index.py`                         | MIME-type or file extension–based dispatch (`.endswith()`/`mimetypes.guess_type()`). Then routed to correct chunking handler. Each handler outputs a JSONL file to `/data/chunked/`.                                  | Modular dispatch layer. Easy to extend with new handlers (e.g., `.xml`, `.epub`).                                                 |
| **Content Hashing / Deduplication**   | `hashlib`                                            | Full file is streamed → `sha256(file_bytes)` becomes `document_id`. Each chunk ID is `chunk_{sha256}_{chunk_index}`.                                                                                                  | SHA256 ensures uniqueness and stability. Streaming hash avoids full memory load.                                                  |
| **S3 I/O**                            | `boto3`                                              | Files pulled using `s3.download_fileobj()` or `s3.get_object()` with stream buffer. Chunked outputs are written using `s3.upload_fileobj()` or multipart upload.                                                      | Supports large files and secure IAM-backed access. Built-in retry, error handling, multipart upload.                              |
| **Entity Linking (Multilingual)**     | `ReFinED`                                            | Each chunk’s `text` passed to `refined.get_entities(text)`. Output merged into `entities[]`. Handles multilingual mentions and coref resolution.                                                                      | High performance on CPU/GPU, fast and light, multilingual disambiguation works across >9 languages.                               |
| **Embedding Generation**              | `elastic/multilingual-e5-small-optimized`            | Chunk’s `text` sent to `AutoModel.from_pretrained(...)` → forward pass for CLS token → vector appended to `.embedding`. Token limit handled via truncation or sliding window.                                         | High performance, distilled architecture. Efficient at production scale.                                                          |
| **Vector Index**                      | `qdrant-client`                                      | Vector with chunk metadata inserted via `qdrant_client.upload_collections()`. Supports `filter`, `metadata`, `payload` for hybrid search. HNSW handles scale well.                                                    | Fast ANN search, persistent, supports metadata filtering. Easy to shard by file/doc/tenant.                                       |
| **Knowledge Graph / Triplet Storage** | `python-arango`                                      | Triplets extracted from chunk (via NER/RE) are inserted as AQL `UPSERT` into document and edge collections. Document key = `chunk_id`.                                                                                | ACID + graph model in one. Good for hybrid KB + RAG backends.                                                                     |
